<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>gesture-recognition: gestureRecognition</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">gesture-recognition
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">gestureRecognition</div>  </div>
</div><!--header-->
<div class="contents">

<p>A module that recognizes in real-time gestures belonging to a predefined set using 3DHOF and HOG descriptors and linear SVMs as classifiers.  
<a href="#details">More...</a></p>
<p>A module that recognizes in real-time gestures belonging to a predefined set using 3DHOF and HOG descriptors and linear SVMs as classifiers. </p>
<h1><a class="anchor" id="intro_sec"></a>
Description</h1>
<p>This module is able to recognize different gestures that belong to a predefined pool. It is possible to save features and train new Support Vector Machines using libsvm. Right now we provide a file to recognize 6 gestures depicted in app/conf/supported_actions_easy.jpg. 3DHOF and HOG are used as descriptors. For further information:</p>
<p>"S. R. Fanello*, I. Gori*, G. Metta, F. Odone Keep It Simple And Sparse: Real-Time Action Recognition. Journal of Machine Learning Research (JMLR), 2013."</p>
<h1><a class="anchor" id="rpc_port"></a>
Commands:</h1>
<p>The commands sent as bottles to the module port /&lt;modName&gt;/rpc are described in the following:</p>
<p><b>REC</b> <br />
format: [rec] <br />
action: the module starts to recognize any action that is being performed.</p>
<p><b>SAVE</b> <br />
format: [save] <br />
action: the module starts saving features on a .txt file located in the directory specified as outDir in the ResourceFinder.</p>
<p><b>STOP</b> <br />
format: [stop] <br />
action: the module stops recognizing or saving.</p>
<h1><a class="anchor" id="lib_sec"></a>
Libraries</h1>
<ul>
<li>YARP libraries.</li>
<li>kinect-wrapper library.</li>
<li>OpenCV library.</li>
</ul>
<h1><a class="anchor" id="portsc_sec"></a>
Ports Created</h1>
<ul>
<li><em>/</em> &lt;modName&gt;/rpc remote procedure call. It always replies something.</li>
<li><em>/</em> &lt;modName&gt;/scores this is the port where the module outputs the result of the recognition procedure.</li>
<li><em>/</em> &lt;modName&gt;/images this port outputs an image containing the player, his skeleton joints and his hands segmented with two different colors.</li>
</ul>
<h1><a class="anchor" id="parameters_sec"></a>
Parameters</h1>
<p>The following are the options that are usually contained in the configuration file, which is gestureRecognition.ini:</p>
<p>&ndash;name <em>name</em> </p><ul>
<li>specify the module name, which is <em>gestureRecognition</em> by default.</li>
</ul>
<p>&ndash;dimension <em>dimension</em> </p><ul>
<li>number of bins for the 3D histogram of flow. The final histogram will be dimension*dimension*dimension.</li>
</ul>
<p>&ndash; outDir <em>outDir</em> </p><ul>
<li>folder where the features will be saved, if the save command is sent to the rpc port.</li>
</ul>
<p>&ndash; showImages <em>showImages</em> </p><ul>
<li>if this is true, depth and skeleton will be visualized.</li>
</ul>
<p>There is another configuration file that should be provided, called SVMModels.ini. It is the file that contains information on the SVMs that have been trained on the gestures it is wanted to recognize. One example file is already provided, and it allows recognizing 6 gestures specified in the file app/conf/supported_actions_easy.jpg. Those gestures can also be used to play the All Gestures You Can game. For further information:</p>
<p>I. Gori, S. R. Fanello, G. Metta, F. Odone All Gestures You Can: A Memory Game Against A Humanoid Robot. In proceedings of IEEE-RAS International Conference on Humanoid Robots, 2012.</p>
<p>The parameters that should be present in the SVMModels.ini file are:</p>
<p>&ndash; numActions <em>numActions</em> </p><ul>
<li>in the group GENERAL, number of trained actions.</li>
</ul>
<p>&ndash; nFeatures <em>nFeatures</em> </p><ul>
<li>in the group GENERAL. It is 2 if 3DHOF and HOG on the whole body are used. If also HOGs on the two hands are used, this parameter should be equal to 4.</li>
</ul>
<p>&ndash; dictionarySize <em>dictionarySize</em> </p><ul>
<li>in the group GENERAL. Not used for now. It is needed if a dictionary learning step has to be applied.</li>
</ul>
<p>&ndash; bufferSize <em>bufferSize</em> </p><ul>
<li>in the group GENERAL. Number of frames to be evaluated at each step.</li>
</ul>
<p>&ndash; frameFeatureSize <em>frameFeatureSize</em> </p><ul>
<li>in the group GENERAL. Size of the final frame descriptor.</li>
</ul>
<p>For each action there is a group called ACTIONX, where X is a number going from 1 to the number of trained actions. In each ACTIONX group there should be the parameter w, which is a list of the coefficients of the trained SVM.</p>
<h1><a class="anchor" id="tested_os_sec"></a>
Tested OS</h1>
<p>Windows, Linux</p>
<dl class="section author"><dt>Author</dt><dd>Ilaria Gori, Sean Ryan Fanello </dd></dl>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
